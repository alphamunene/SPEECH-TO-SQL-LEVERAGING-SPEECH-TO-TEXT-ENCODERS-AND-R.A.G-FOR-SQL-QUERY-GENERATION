
---

````markdown
# Speech2SQL ðŸš€

Speech2SQL is a **FastAPI application** that allows you to query any database using **natural language** or **voice commands**. It integrates a **fine-tuned Seq2Seq LLM**, **RAG (Retrieval-Augmented Generation)**, and **Whisper** for speech-to-text transcription, enabling a **voice-driven SQL assistant**.  

It works with **SQLite, MySQL, and PostgreSQL** and supports multiple databases.

---

## Features

- **Natural Language â†’ SQL**  
  Convert English queries into executable SQL automatically using a fine-tuned LLM.  

- **Speech â†’ SQL**  
  Upload audio files and transcribe them into SQL queries using OpenAI Whisper.  

- **RAG-based schema retrieval**  
  Automatically extracts database schemas, embeds them with **Sentence Transformers**, and uses **FAISS** to retrieve relevant tables/columns for better query accuracy.  

- **Multi-database support**  
  Connect SQLite, MySQL, or PostgreSQL databases dynamically.  

- **Query history**  
  Tracks executed queries with results, timestamped for easy reference.  

- **Admin controls**  
  Password-protected endpoints to upload databases and rebuild the FAISS schema index.  

- **Schema exploration**  
  View columns, primary/foreign keys, and sample data for each table.

---

## How RAG Works in Speech2SQL

RAG (**Retrieval-Augmented Generation**) improves SQL generation accuracy by providing relevant context from the database schema:  

1. When a user inputs a query, the system **encodes the query** using a Sentence Transformer.  
2. It searches the **FAISS index** built from table schemas to find the **most relevant tables and columns**.  
3. These top schema matches are combined with the user query as context for the **LLM**, which generates the final SQL query.  
4. If the query comes from audio, Whisper transcribes it first, then the same RAG + LLM pipeline runs.  

This ensures that generated SQL is **aware of actual table names and column names**, reducing errors.

---

## Installation

1. **Clone the repository**

```bash
git clone https://github.com/<your_username>/Speech2SQL.git
cd Speech2SQL
````

2. **Create a virtual environment and activate**

```bash
python -m venv venv
# Windows
venv\Scripts\activate
# Linux / macOS
source venv/bin/activate
```

3. **Install dependencies**

```bash
pip install -r requirements.txt
```

**Additional audio dependencies**

```bash
pip install soundfile pydub ffmpeg-python
```

> âš ï¸ **FFmpeg is required for audio transcription**
>
> * Windows: [Gyan.dev FFmpeg](https://www.gyan.dev/ffmpeg/builds/) or [BtbN builds](https://github.com/BtbN/FFmpeg-Builds/releases)
> * macOS: `brew install ffmpeg`
> * Linux: `sudo apt install ffmpeg`

4. **Place your fine-tuned LLM model** in the `trainedmodel/` folder.

* Should include tokenizer and model files (`pytorch_model.bin`, `config.json`, etc.)

---

## Requirements

```text
fastapi
uvicorn
torch
transformers
sentence-transformers
faiss-cpu
sqlalchemy
whisper
numpy
pydub
soundfile
ffmpeg-python
```

---

## Running the App

```bash
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

Open [http://localhost:8000](http://localhost:8000) in your browser.

---

## API Endpoints

### Database Management

| Endpoint                    | Method | Description                                       |
| --------------------------- | ------ | ------------------------------------------------- |
| `/api/connect_db`           | POST   | Connect to a database using connection string     |
| `/api/upload_db`            | POST   | Upload a new SQLite database (`.db` or `.sqlite`) |
| `/api/rebuild_schema_index` | POST   | Rebuild FAISS schema index                        |

### Querying

| Endpoint           | Method | Description                                              |
| ------------------ | ------ | -------------------------------------------------------- |
| `/api/query_nl`    | POST   | Execute natural language query (returns SQL and results) |
| `/api/view_sql`    | POST   | Preview SQL generated by LLM                             |
| `/api/show-tables` | GET    | Quick table view (limit optional)                        |
| `/api/view_schema` | GET    | View schema with keys and sample rows                    |

### Audio

| Endpoint            | Method | Description                                   |
| ------------------- | ------ | --------------------------------------------- |
| `/api/upload_audio` | POST   | Upload audio and transcribe to text (Whisper) |

### History

| Endpoint                    | Method | Description                   |
| --------------------------- | ------ | ----------------------------- |
| `/api/history`              | GET    | Get query history (paginated) |
| `/api/history/{history_id}` | DELETE | Delete a single history entry |
| `/api/history`              | DELETE | Delete all history entries    |

---

## Usage Examples

### Natural Language Query

```json
POST /api/query_nl
{
    "query": "Show all sales greater than 500 in the last month"
}
```

### Upload Audio

```bash
curl -X POST "http://localhost:8000/api/upload_audio" \
  -F "file=@sample_audio.wav"
```

### Upload Database

```bash
curl -X POST "http://localhost:8000/api/upload_db" \
  -H "x-admin-auth: admin" \
  -F "file=@sales.db"
```

---

## Project Structure

```
Speech2SQL/
â”‚
â”œâ”€ backend                # FastAPI backend
â”œâ”€ trainedmodel/          # Fine-tuned LLM + tokenizer
â”œâ”€ rag_store/             # FAISS index + schemas (auto-generated)
â”œâ”€ static/                # Frontend static files
â”œâ”€ templates/             # HTML templates
â”œâ”€ history.db             # SQLite for query history
â”œâ”€ requirements.txt
â””â”€ README.md
```

---

## Notes

* Ensure your LLM model can handle **sequence-to-sequence SQL generation**.
* FAISS is used for **RAG retrieval** of schema context.
* Whisper base model is used for **faster audio transcription**.
* Admin password is `"admin"` by default; change it in `main.py`.

---

## License

MIT License â€“ see `LICENSE` file.

```

---


```
